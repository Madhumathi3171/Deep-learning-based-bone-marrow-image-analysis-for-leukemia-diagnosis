()(input_feature)
class CBAMBlock(layers.Layer):
    def __init__(self, reduction=16, **kwargs):
        super(CBAMBlock, self).__init__(**kwargs)
        self.reduction = reduction

    def build(self, input_shape):
        channel = input_shape[-1]
        self.shared_mlp = tf.keras.Sequential([
            layers.Dense(channel // self.reduction, activation='relu'),
            layers.Dense(channel)
        ])
        # Create the Conv2D layer for spatial attention here
        self.spatial_conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')
        super(CBAMBlock, self).build(input_shape)

    def call(self, input_feature):
        channel = input_feature.shape[-1]

        # ---- Channel Attention ----
        avg_pool = layers.GlobalAveragePooling2D()(input_feature)
        max_pool = layers.GlobalMaxPooling2D
        avg_out = self.shared_mlp(avg_pool)
        max_out = self.shared_mlp(max_pool)

        channel_attention = layers.Activation('sigmoid')(avg_out + max_out)
        channel_attention = layers.Reshape((1, 1, channel))(channel_attention)

        x = layers.Multiply()([input_feature, channel_attention])

        # ---- Spatial Attention ----
        avg_pool_spatial = tf.reduce_mean(x, axis=-1, keepdims=True)
        max_pool_spatial = tf.reduce_max(x, axis=-1, keepdims=True)

        concat = layers.Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])
        spatial_attention = self.spatial_conv(concat)  # Use the pre-built layer

        x = layers.Multiply()([x, spatial_attention])
        return x

    def get_config(self):
        config = super(CBAMBlock, self).get_config()
        config.update({"reduction": self.reduction})
        return config
